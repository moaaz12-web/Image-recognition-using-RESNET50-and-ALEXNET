{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moaaz12-web/Image-recognition-using-RESNET50-and-ALEXNET/blob/main/AlexNet_training_and_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip \"/content/dataset_cattle_alexnet.zip\" -d \"./dataset\""
      ],
      "metadata": {
        "id": "vupw8msn0ruB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de4015e9-41b0-480d-92ab-37ab3fe170e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/dataset_cattle_alexnet.zip\n",
            "   creating: ./dataset/dataset_cattle_alexnet/\n",
            "   creating: ./dataset/dataset_cattle_alexnet/test/\n",
            "  inflating: ./dataset/dataset_cattle_alexnet/test/36.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/test/37.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/test/38.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/test/39.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/test/40.jpg  \n",
            "   creating: ./dataset/dataset_cattle_alexnet/train/\n",
            "   creating: ./dataset/dataset_cattle_alexnet/train/cattle/\n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/1.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/10.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/11.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/12.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/13.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/14.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/15.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/16.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/17.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/18.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/19.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/2.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/20.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/21.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/22.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/23.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/24.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/25.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/26.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/27.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/28.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/29.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/3.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/30.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/31.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/32.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/33.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/34.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/35.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/4.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/5.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/6.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/7.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/8.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/train/cattle/9.jpg  \n",
            "   creating: ./dataset/dataset_cattle_alexnet/val/\n",
            "   creating: ./dataset/dataset_cattle_alexnet/val/cattle/\n",
            "  inflating: ./dataset/dataset_cattle_alexnet/val/cattle/1.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/val/cattle/10.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/val/cattle/2.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/val/cattle/3.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/val/cattle/4.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/val/cattle/5.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/val/cattle/6.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/val/cattle/7.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/val/cattle/8.jpg  \n",
            "  inflating: ./dataset/dataset_cattle_alexnet/val/cattle/9.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU2kGS0tc979",
        "outputId": "a9602589-fbfb-41d5-a26f-e0bbc067c474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone \"https://github.com/madsendennis/notebooks.git\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIiY8XLKu98K",
        "outputId": "9364c210-eb12-41e3-f8fc-4738df5232fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'notebooks'...\n",
            "remote: Enumerating objects: 273, done.\u001b[K\n",
            "remote: Counting objects: 100% (273/273), done.\u001b[K\n",
            "remote: Compressing objects: 100% (262/262), done.\u001b[K\n",
            "remote: Total 273 (delta 16), reused 262 (delta 9), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (273/273), 15.59 MiB | 23.20 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oT89I78tTPw"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "from torchsummary import summary\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Applying Transforms to the Data\n",
        "image_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'valid': transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "i5lZsqHctmQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Data\n",
        "\n",
        "# Set train and valid directory paths\n",
        "\n",
        "dataset = '/content/dataset_cattle_alexnet'\n",
        "\n",
        "train_directory = os.path.join(dataset, 'train')\n",
        "valid_directory = os.path.join(dataset, 'valid')\n",
        "\n",
        "# Batch size\n",
        "bs = 32\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(os.listdir(valid_directory))  #10#2#257\n",
        "print(num_classes)\n",
        "\n",
        "# Load Data from folders\n",
        "data = {\n",
        "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
        "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid'])\n",
        "}\n",
        "\n",
        "# Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
        "idx_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
        "print(idx_to_class)\n",
        "\n",
        "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
        "train_data_size = len(data['train'])\n",
        "valid_data_size = len(data['valid'])\n",
        "\n",
        "# Create iterators for the Data loaded using DataLoader module\n",
        "train_data_loader = DataLoader(data['train'], batch_size=bs, shuffle=True)\n",
        "valid_data_loader = DataLoader(data['valid'], batch_size=bs, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x754F_TWtqLq",
        "outputId": "959cffa5-21ff-4e4c-eb1a-dd48f53f8832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "{0: 'cattle'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alexnet = models.alexnet(pretrained=True)\n",
        "alexnet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1UC6cg3t1F6",
        "outputId": "6002dbcd-af26-4073-bb0a-fb3e410c2404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze model parameters\n",
        "for param in alexnet.parameters():\n",
        "    param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "oB8o-4T5t43I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtxTGCJtnEwB",
        "outputId": "26d279fa-be0d-4ff8-b8ca-2c57051010c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Change the final layer of AlexNet Model for Transfer Learning\n",
        "alexnet.classifier[6] = nn.Linear(4096, num_classes)\n",
        "alexnet.classifier.add_module(\"7\", nn.LogSoftmax(dim = 1))\n",
        "alexnet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjhp0OLVt6kO",
        "outputId": "95c3f670-072a-40fe-a7ae-5e3ae69b705f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=1, bias=True)\n",
              "    (7): LogSoftmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alexnet = alexnet.to('cuda')"
      ],
      "metadata": {
        "id": "JZjygEt4vjrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(alexnet, (3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dklKsBz4t85M",
        "outputId": "15f1722e-e3d4-45ed-84ee-9c720219abae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 55, 55]          23,296\n",
            "              ReLU-2           [-1, 64, 55, 55]               0\n",
            "         MaxPool2d-3           [-1, 64, 27, 27]               0\n",
            "            Conv2d-4          [-1, 192, 27, 27]         307,392\n",
            "              ReLU-5          [-1, 192, 27, 27]               0\n",
            "         MaxPool2d-6          [-1, 192, 13, 13]               0\n",
            "            Conv2d-7          [-1, 384, 13, 13]         663,936\n",
            "              ReLU-8          [-1, 384, 13, 13]               0\n",
            "            Conv2d-9          [-1, 256, 13, 13]         884,992\n",
            "             ReLU-10          [-1, 256, 13, 13]               0\n",
            "           Conv2d-11          [-1, 256, 13, 13]         590,080\n",
            "             ReLU-12          [-1, 256, 13, 13]               0\n",
            "        MaxPool2d-13            [-1, 256, 6, 6]               0\n",
            "AdaptiveAvgPool2d-14            [-1, 256, 6, 6]               0\n",
            "          Dropout-15                 [-1, 9216]               0\n",
            "           Linear-16                 [-1, 4096]      37,752,832\n",
            "             ReLU-17                 [-1, 4096]               0\n",
            "          Dropout-18                 [-1, 4096]               0\n",
            "           Linear-19                 [-1, 4096]      16,781,312\n",
            "             ReLU-20                 [-1, 4096]               0\n",
            "           Linear-21                    [-1, 1]           4,097\n",
            "       LogSoftmax-22                    [-1, 1]               0\n",
            "================================================================\n",
            "Total params: 57,007,937\n",
            "Trainable params: 4,097\n",
            "Non-trainable params: 57,003,840\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 8.37\n",
            "Params size (MB): 217.47\n",
            "Estimated Total Size (MB): 226.41\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define Optimizer and Loss Function\n",
        "loss_func = nn.NLLLoss()\n",
        "optimizer = optim.Adam(alexnet.parameters())\n",
        "optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiXHNGd_uAEL",
        "outputId": "eae1ff67-e576-4ed1-b8aa-26340b569737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    capturable: False\n",
              "    differentiable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.001\n",
              "    maximize: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "\n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "\n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "\n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "\n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "\n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_data_loader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "\n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "           # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(valid_data_loader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size\n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_valid_loss = valid_loss/valid_data_size\n",
        "        avg_valid_acc = valid_acc/valid_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
        "\n",
        "        epoch_end = time.time()\n",
        "\n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch+1, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n",
        "\n",
        "        # Save if the model has best accuracy till now\n",
        "        #torch.save(model, dataset+'_model_'+str(epoch)+'.pt')\n",
        "\n",
        "    return model, history\n"
      ],
      "metadata": {
        "id": "UAUfDUE3uEoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "num_epochs = 50\n",
        "trained_model, history = train_and_validate(alexnet, loss_func, optimizer, num_epochs)\n",
        "\n",
        "torch.save(history, dataset+'_history.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPk0OANGukz7",
        "outputId": "56ead897-e9dc-45f2-bbf7-0b431d23b2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/50\n",
            "Epoch : 001, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 1.1038s\n",
            "Epoch: 2/50\n",
            "Epoch : 002, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.5508s\n",
            "Epoch: 3/50\n",
            "Epoch : 003, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.5528s\n",
            "Epoch: 4/50\n",
            "Epoch : 004, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.4704s\n",
            "Epoch: 5/50\n",
            "Epoch : 005, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3570s\n",
            "Epoch: 6/50\n",
            "Epoch : 006, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3654s\n",
            "Epoch: 7/50\n",
            "Epoch : 007, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3518s\n",
            "Epoch: 8/50\n",
            "Epoch : 008, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3417s\n",
            "Epoch: 9/50\n",
            "Epoch : 009, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3657s\n",
            "Epoch: 10/50\n",
            "Epoch : 010, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3466s\n",
            "Epoch: 11/50\n",
            "Epoch : 011, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3462s\n",
            "Epoch: 12/50\n",
            "Epoch : 012, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3668s\n",
            "Epoch: 13/50\n",
            "Epoch : 013, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3587s\n",
            "Epoch: 14/50\n",
            "Epoch : 014, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3610s\n",
            "Epoch: 15/50\n",
            "Epoch : 015, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3415s\n",
            "Epoch: 16/50\n",
            "Epoch : 016, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3508s\n",
            "Epoch: 17/50\n",
            "Epoch : 017, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3601s\n",
            "Epoch: 18/50\n",
            "Epoch : 018, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3455s\n",
            "Epoch: 19/50\n",
            "Epoch : 019, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3608s\n",
            "Epoch: 20/50\n",
            "Epoch : 020, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3534s\n",
            "Epoch: 21/50\n",
            "Epoch : 021, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3391s\n",
            "Epoch: 22/50\n",
            "Epoch : 022, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3435s\n",
            "Epoch: 23/50\n",
            "Epoch : 023, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3508s\n",
            "Epoch: 24/50\n",
            "Epoch : 024, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3451s\n",
            "Epoch: 25/50\n",
            "Epoch : 025, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3513s\n",
            "Epoch: 26/50\n",
            "Epoch : 026, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3574s\n",
            "Epoch: 27/50\n",
            "Epoch : 027, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3478s\n",
            "Epoch: 28/50\n",
            "Epoch : 028, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3550s\n",
            "Epoch: 29/50\n",
            "Epoch : 029, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3570s\n",
            "Epoch: 30/50\n",
            "Epoch : 030, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3477s\n",
            "Epoch: 31/50\n",
            "Epoch : 031, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3457s\n",
            "Epoch: 32/50\n",
            "Epoch : 032, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3577s\n",
            "Epoch: 33/50\n",
            "Epoch : 033, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.5056s\n",
            "Epoch: 34/50\n",
            "Epoch : 034, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.5068s\n",
            "Epoch: 35/50\n",
            "Epoch : 035, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.5126s\n",
            "Epoch: 36/50\n",
            "Epoch : 036, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.5299s\n",
            "Epoch: 37/50\n",
            "Epoch : 037, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.5184s\n",
            "Epoch: 38/50\n",
            "Epoch : 038, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.5204s\n",
            "Epoch: 39/50\n",
            "Epoch : 039, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.5236s\n",
            "Epoch: 40/50\n",
            "Epoch : 040, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.5162s\n",
            "Epoch: 41/50\n",
            "Epoch : 041, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.5181s\n",
            "Epoch: 42/50\n",
            "Epoch : 042, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.5078s\n",
            "Epoch: 43/50\n",
            "Epoch : 043, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.7853s\n",
            "Epoch: 44/50\n",
            "Epoch : 044, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3547s\n",
            "Epoch: 45/50\n",
            "Epoch : 045, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3531s\n",
            "Epoch: 46/50\n",
            "Epoch : 046, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3511s\n",
            "Epoch: 47/50\n",
            "Epoch : 047, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3614s\n",
            "Epoch: 48/50\n",
            "Epoch : 048, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3493s\n",
            "Epoch: 49/50\n",
            "Epoch : 049, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3469s\n",
            "Epoch: 50/50\n",
            "Epoch : 050, Training: Loss: 0.0000, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.0000, Accuracy: 100.0000%, Time: 0.3551s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = np.array(history)\n",
        "\n",
        "plt.plot(history[:,0:2])\n",
        "plt.legend(['Tr Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0,1)\n",
        "plt.savefig(dataset+'_loss_curve.png')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history[:,2:4])\n",
        "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "plt.savefig(dataset+'_accuracy_curve.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KNJFt257RAqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to a file\n",
        "torch.save(trained_model.state_dict(), 'my_model.pth')"
      ],
      "metadata": {
        "id": "LYMMP_6sd_ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the model from the file\n",
        "# # loaded_model = AlexNet()\n",
        "alexnet.load_state_dict(torch.load('my_model.pth'))\n",
        "\n",
        "\n",
        "# # Load the model from the file\n",
        "# # loaded_model = AlexNet()\n",
        "# # loaded_model.load_state_dict(torch.load('my_model.pth'))\n",
        "\n",
        "# # loaded_model.load_state_dict(torch.load('my_model.pth'))\n"
      ],
      "metadata": {
        "id": "XoxdoeMyeavr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e2e943-30c0-40f1-df6e-ac0438e849e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, test_image_name):\n",
        "\n",
        "    '''\n",
        "    Function to predict the class of a single test image\n",
        "    Parameters\n",
        "        :param model: Model to test\n",
        "        :param test_image_name: Test image\n",
        "    '''\n",
        "\n",
        "    transform = image_transforms['test']\n",
        "\n",
        "    test_image = Image.open(test_image_name)\n",
        "    plt.imshow(test_image)\n",
        "\n",
        "    test_image_tensor = transform(test_image)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224).cuda()\n",
        "    else:\n",
        "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Model outputs log probabilities\n",
        "        out = model(test_image_tensor)\n",
        "        ps = torch.exp(out)\n",
        "        print(out)\n",
        "\n",
        "\n",
        "        # Move tensors to CPU before converting to numpy\n",
        "        topclass_cpu = out.cpu().numpy()\n",
        "        topk_cpu = ps.cpu().numpy()\n",
        "\n",
        "        for i in range(1):\n",
        "            print(\"Prediction\", i+1, \":\", idx_to_class[int(topclass_cpu[0][i])])"
      ],
      "metadata": {
        "id": "hNoM7pDMuyXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGrqZMrMWf_C",
        "outputId": "bc67da61-756e-4619-eec8-945ef7d7fce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'cattle'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(alexnet, '/content/dd.jpg')"
      ],
      "metadata": {
        "id": "7CdVFmAOu2g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V4wfTUORaHl9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}